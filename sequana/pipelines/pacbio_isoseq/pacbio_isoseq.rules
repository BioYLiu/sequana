"""Isoseq pipeline

Affiliation: Institut Pasteur @ 2019

This pipeline is part of Sequana software (sequana.readthedocs.io)
"""
import os
import sequana
from os.path import join
import pandas as pd
from sequana import snaketools as sm
sm.init("pacbio_isoseq.rules", globals())


# This must be defined before the include
configfile: "config.yaml"
__snakefile__ = srcdir(__snakefile__)

exec(open(sequana.modules["minimap2_mapping_dynamic"], "r").read())

manager = sm.PipelineManager("pacbio_isoseq", config, fastq=False)
__data__input = manager.getrawdata()



#Manage design file for SMRTCell merging
__design__ = manager.config.general.design_file
design = pd.read_csv(__design__, header=0, sep='\t')

conds = set(list(design['Prefix']))

def getBamFilesByCond(wildcards):
    ALL = expand(__refine__output)
    COND_dict = {}
    for cond in conds:
        COND_dict[cond] = []
        for file in ALL:
            if cond in file:
                COND_dict[cond].append(file)

    return COND_dict[wildcards["COND"]]

#Generate circular consensus sequence (CCS)
__ccs__input = __data__input
__ccs__output =  "0-Preprocess/{sample}.ccs.bam"
__ccs__log_std = "0-Preprocess/Logs/{sample}.ccs.out"
__ccs__log_err = "0-Preprocess/Logs/{sample}.ccs.err"
__ccs__report_file = "0-Preprocess/Logs/{sample}_report.txt"
include: sm.modules['ccs']
#expected_output.extend(expand(__ccs__output, sample=manager.samples))


#Primer removal and demultiplexing
__lima__input = __ccs__output
__lima__output = "0-Preprocess/{sample}.demux.ccs.bam"
__lima__barcoded_primers = config["lima"]["primers"]
__lima__barcoded_options = " --isoseq --no-pbi "
__lima__log_std = "0-Preprocess/Logs/{sample}.lima.out"
__lima__log_err = "0-Preprocess/Logs/{sample}.lima.err"
include: sm.modules['lima']
#expected_output.extend(expand(__lima__output, sample=manager.samples))


#PolyA trimming
__refine__input = __lima__output
__refine__output = "0-Preprocess/{sample}.flnc.bam"
__refine__log_std = "0-Preprocess/Logs/{sample}.refine.out"
__refine__log_err = "0-Preprocess/Logs/{sample}.refine.err"
include: sm.modules['refine']
#expected_output.extend(expand(__refine__output, sample=manager.samples))


if manager.config.general.merge_smrtcell:
    #merge SMRTcell if needed
    __dataset__input_bam = getBamFilesByCond
    __dataset__log = "0-Preprocess/Logs/{sample}.merge.out"
    __dataset__output = "blabla{COND}blabla"
    include: sm.modules["dataset"]
    expected_output.extend(expand(__dataset__output, COND=conds))
    ## TODO how to link merge xml and flnc.bam to cluster step ?


#Clustering and transcript clean up
__cluster__input = __refine__output
__cluster__output = "1-HQ_transcripts/Logs/{sample}_unpolished.bam"
__cluster__log_std = "1-HQ_transcripts/Logs/{sample}.cluster.out"
__cluster__log_err = "1-HQ_transcripts/Logs/{sample}.cluster.err"
include: sm.modules['cluster']
#expected_output.extend(expand(__cluster__output, sample=manager.samples))

#Polishing
__polish__input_bam = __cluster__output
__polish__input_subreads = __data__input
__polish__output = "1-HQ_transcripts/{sample}_polished.bam"
__polish__output_hq = "1-HQ_transcripts/{sample}.hq.fastq.gz"
__polish__output_lq = "1-HQ_transcripts/{sample}.lq.fastq.gz"
__polish__log_std = "1-HQ_transcripts/Logs/{sample}.polish.out"
__polish__log_err = "1-HQ_transcripts/Logs/{sample}.polish.err"
include: sm.modules['polish']
expected_output.extend(expand(__polish__output, sample=manager.samples))

if manager.config.general.spikes:
    __minimap2_mapping_spikes__input = __polish__output_hq
    __minimap2_mapping_spikes__logs = "2-Spikes_analysis/Logs/{sample}_spikes_mapping.out"
    __minimap2_mapping_spikes__sort = "2-Spikes_analysis/{sample}_spikes_sort.bam"
    __minimap2_mapping_spikes__output_bam = "2-Spikes_analysis/{sample}_spikes.bam"
    __minimap2_mapping_spikes__ref = config["general"]["spikes_file"]
    include: minimap2_mapping_dynamic("spikes", manager)
    expected_output.extend(expand(__minimap2_mapping_spikes__sort, sample=manager.samples))

    #extract no spike reads
    __get_no_spikes__input = __minimap2_mapping_spikes__sort
    __get_no_spikes__output = "2-Spikes_analysis/{sample}_no-spikes.fastq.gz"
    __get_no_spikes__log = "2-Spikes_analysis/Logs/{sample}_no-spikes.out"
    include: sm.modules['get_no_spikes']
    expected_output.extend(expand(__get_no_spikes__output, sample=manager.samples))

    #replace all hq by all hq that don't match spikes
    __polish__output_hq = __get_no_spikes__output



#mapping with minimap2
if manager.config.minimap2_mapping.do:
    # todo include genome name ?
    __minimap2_mapping_genome__input = __polish__output_hq
    __minimap2_mapping_genome__logs = "2-Transcripts_analysis/Logs/{sample}_HQ_mapping.out"
    __minimap2_mapping_genome__sort = "2-Transcripts_analysis/{sample}_HQ_sort.bam"
    __minimap2_mapping_genome__output_bam = "2-Transcripts_analysis/{sample}_HQ.bam"
    __minimap2_mapping_genome__ref = config["general"]["genome_file"]
    include: minimap2_mapping_dynamic("genome", manager)
    expected_output.extend(expand(__minimap2_mapping_genome__sort, sample=manager.samples))

    if manager.config.general.annotation_file:
        __bedtools_coverage__input_bam = __minimap2_mapping_genome__output_bam
        __bedtools_coverage__input_annotation = config["general"]["annotation_file"]
        __bedtools_coverage__output = "2-Transcripts_analysis/{sample}_HQ_coverage.bed"
        __bedtools_coverage__log = "2-Transcripts_analysis/Logs/{sample}_HQ_coverage.out"
        include: sm.modules['bedtools_coverage']
        expected_output.extend(expand(__bedtools_coverage__output, sample=manager.samples))





# Include rule graph for each sample
__rulegraph__input = __snakefile__
__rulegraph__output = "rulegraph/rulegraph.svg"
__rulegraph__mapper = {"fastqc_samples": "fastqc_samples/"}
include: sm.modules['rulegraph']
expected_output.extend([__rulegraph__output])


# Add Conda
__conda__output = "requirements.txt"
include: sm.modules['conda']   # Create requirements.txt(dependencies)
expected_output.extend([__conda__output])


# Those rules takes a couple of seconds so no need for a cluster
localrules: conda, rulegraph

rule pacbio_isoseq:
    input: expected_output


onsuccess:
    import os
    # Create plots about stats
    sm.plot_stats(N=len(manager.samples))

    # Main directory
    report_dir_format = "%(proj)s/report_pacbio_isoseq_%(proj)s"
    for proj in manager.samples.keys():
        report_dir = report_dir_format % {"proj": proj}
        try:os.mkdir(report_dir)
        except:pass

        shell("cp %s %s" % (__snakefile__, report_dir))
        #shell("cp rulegraph.svg %s/rulegraph.svg" % (report_dir))
        shell("cp config.yaml %s" % report_dir)
        shell("cp requirements.txt %s" % report_dir)
        shell("cp snakemake_stats.png %s" % report_dir)
        try: os.mkdir("cluster_logs")
        except:pass

        try: shell("mv slurm* cluster_logs/")
        except: pass

        # Create a cleanup python file to clean a sub-directory
        sm.create_cleanup(proj)

    sm.OnSuccess()() # create instance to create main cleanup


onerror:
    print("An error occurred. See message above.")


